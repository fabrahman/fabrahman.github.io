<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Faeze Brahman</title> <meta name="author" content="Faeze Brahman"> <meta name="description" content="publications by categories in reversed chronological order. &lt;br&gt; * and + denote co-first and co-second authors."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://fabrahman.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Faeze </span>Brahman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">mentees</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. <br> * and + denote co-first and co-second authors.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ACL</a></abbr></div> <div id="yin2024agent" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2311.05657" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Agent Lumos: Unified and Modular Training for Open-Source Language Agents</a></div> <div class="author"> Da Yin, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin</div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/GXimingLu/IPA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code/Data</a> <a href="https://github.com/GXimingLu/IPA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Webpage</a> <a href="https://github.com/GXimingLu/IPA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Press</a> <a href="https://allenai.github.io/lumos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">yin2024agent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Agent Lumos: Unified and Modular Training for Open-Source Language Agents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yin, Da and Brahman, Faeze and Ravichander, Abhilasha and Chandu, Khyathi and Chang, Kai-Wei and Choi, Yejin and Lin, Bill Yuchen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of ACL}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.05657}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2311.05657}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/GXimingLu/IPA}</span><span class="p">,</span>
  <span class="na">press</span> <span class="p">=</span> <span class="s">{https://www.marktechpost.com/2024/04/01/lumos-an-open-source-generalizable-language-agent-training-framework/}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NAACL</a></abbr></div> <div id="tian2023macgyver" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2311.09682" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">MacGyver: Are Large Language Models Creative Problem Solvers?</a></div> <div class="author"> Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, and <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span> </div> <div class="periodical"> <em>In Proceedings of NAACL</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/allenai/MacGyver" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting. The setting requires circumventing a cognitive bias known in psychology as ”functional fixedness” to use familiar objects in innovative or unconventional ways. To this end, we create MacGyver, an automatically generated dataset consisting of 1,600 real-world problems that deliberately trigger functional fixedness and require thinking ’out-of-the-box’. We then present our collection of problems to both LLMs and humans to compare and contrast their problem-solving abilities. We show that MacGyver is challenging for both groups, but in unique and complementary ways. For example, humans typically excel in solving problems that they are familiar with but may struggle with tasks requiring domain-specific knowledge, leading to a higher variance. On the other hand, LLMs, being exposed to a variety of highly specialized knowledge, attempt broader problems but are prone to overconfidence and propose actions that are physically infeasible or inefficient. We also provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work provides insight into the creative problem-solving capabilities of humans and AI and illustrates how psychological paradigms can be extended into large-scale tasks for comparing humans and machines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tian2023macgyver</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MacGyver: Are Large Language Models Creative Problem Solvers?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tian, Yufei and Ravichander, Abhilasha and Qin, Lianhui and Bras, Ronan Le and Marjieh, Raja and Peng, Nanyun and Choi, Yejin and Griffiths, Thomas L. and Brahman, Faeze}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of NAACL}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.09682}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2311.09682}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NAACL</a></abbr></div> <div id="jung2023impossible" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2305.16635" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Impossible Distillation: from Low-Quality Model to High-Quality Dataset &amp; Model for Summarization and Paraphrasing</a></div> <div class="author"> Jaehun Jung, Peter West, Liwei Jiang, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi</div> <div class="periodical"> <em>In Proceedings of NAACL</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jung2023impossible</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Impossible Distillation: from Low-Quality Model to High-Quality Dataset \&amp; Model for Summarization and Paraphrasing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman, Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2305.16635}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of NAACL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2305.16635}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NAACL</a></abbr></div> <div id="zhao2023uncommonsense" class="col-sm-8"> <div class="title"><a href="" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;">UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations</a></div> <div class="author"> Wenting Zhao, Justin T Chiu, Jena D. Hwang, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang Lorraine Li, and Alane Suhr</div> <div class="periodical"> <em>In Proceedings of NAACL</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhao2023uncommonsense</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Wenting and Chiu, Justin T and Hwang, Jena D. and Brahman, Faeze and Hessel, Jack and Choudhury, Sanjiban and Choi, Yejin and Li, Xiang Lorraine and Suhr, Alane}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of NAACL}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.08469}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ICLR</a></abbr></div> <div id="west2023generative" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2311.00059" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">The Generative AI Paradox: "What It Can Create, It May Not Understand"</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman*</span>, Peter West*, Ximing Lu*, Nouha Dziri*, Linjie Li*, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, and Yejin Choi</div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today’s generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon – and can therefore exceed – their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models’ generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">west2023generative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Generative AI Paradox: "What It Can Create, It May Not Understand"}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brahman*, Faeze and West*, Peter and Lu*, Ximing and Dziri*, Nouha and Li*, Linjie and Hwang, Jena D. and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi and Newman, Benjamin and Koh, Pang Wei and Ettinger, Allyson and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.00059}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2311.00059}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.AI}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ICLR</a></abbr></div> <div id="Brahman2023PlaSma" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2305.19472" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Chandra Bhagavatula, Valentina Pyatkin*, Jena D. Hwang*, Xiang Lorraine Li, Hirona J. Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, and Yejin Choi</div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/allenai/PlaSma" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. ’scheduling a doctor’s appointment without a phone’. While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactual Planning, that requires a revision of a plan to cope with a counterfactual situation. In both the original and counterfactual setting, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models’ capabilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Brahman2023PlaSma</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brahman, Faeze and Bhagavatula, Chandra and Pyatkin*, Valentina and Hwang*, Jena D. and Li, Xiang Lorraine and Arai, Hirona J. and Sanyal, Soumya and Sakaguchi, Keisuke and Ren, Xiang and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ArXiv preprint}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2305.19472}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ICLR</a></abbr></div> <div id="baheti2023improving" class="col-sm-8"> <div class="title"><a href="" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;">Improving Language Models with Advantage-based Offline Policy Gradients</a></div> <div class="author"> Ashutosh Baheti, Ximing Lu, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Ronan Le Bras, Maarten Sap, and Mark Riedl</div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/abaheti95/LoL-RL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM’s internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">baheti2023improving</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Language Models with Advantage-based Offline Policy Gradients}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Baheti, Ashutosh and Lu, Ximing and Brahman, Faeze and Bras, Ronan Le and Sap, Maarten and Riedl, Mark}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2305.14718}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">arXiv</a></abbr></div> <div id="jung2024informationtheoretic" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2403.13780" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Information-Theoretic Distillation for Reference-less Summarization</a></div> <div class="author"> Jaehun Jung, Ximing Lu, Liwei Jiang, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Peter West, Pang Wei Koh, and Yejin Choi</div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ACM</a></abbr></div> <div id="chakrabarty2023creativity" class="col-sm-8"> <div class="title"><a href="" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;">Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers</a></div> <div class="author"> Tuhin Chakrabarty*, Vishakh Padmakumar*, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, and Smaranda Muresan</div> <div class="periodical"> <em>In ACM Conference on Creativity &amp; Cognition</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chakrabarty2023creativity</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chakrabarty*, Tuhin and Padmakumar*, Vishakh and Brahman, Faeze and Muresan, Smaranda}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2309.12570}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM Conference on Creativity &amp; Cognition}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.HC}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="lu2023inference" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2023.emnlp-main.424" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning</a></div> <div class="author"> Ximing Lu, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Peter West, Jaehun Jang, Khyathi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, and  others</div> <div class="periodical"> <em>In Proceedings of EMNLP</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/GXimingLu/IPA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lu2023inference</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lu, Ximing and Brahman, Faeze and West, Peter and Jang, Jaehun and Chandu, Khyathi and Ravichander, Abhilasha and Qin, Lianhui and Ammanabrolu, Prithviraj and Jiang, Liwei and Ramnath, Sahana and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EMNLP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.424}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="rao2023moral" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2310.15431" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations</a></div> <div class="author"> Kavel Rao, Liwei Jiang, Valentina Pyatkin, Yuling Gu, Niket Tandon, Nouha Dziri, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, and Yejin Choi</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios. We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a student model that produces defeasible contexts with improved validity, diversity, and defeasibility. From this model we distill a high-quality dataset, δ-Rules-of-Thumb, of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions rated highly by human annotators 85.9% to 99.8% of the time. Using δ-RoT we obtain a final student model that wins over all intermediate student models by a notable margin.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rao2023moral</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rao, Kavel and Jiang, Liwei and Pyatkin, Valentina and Gu, Yuling and Tandon, Niket and Dziri, Nouha and Brahman, Faeze and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2310.15431}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="hallinan2023steer" class="col-sm-8"> <div class="title"><a href="" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;">STEER: Unified Style Transfer with Expert Reinforcement</a></div> <div class="author"> Skyler Hallinan, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Ximing Lu, Jaehun Jung, Sean Welleck, and Yejin Choi</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hallinan2023steer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{STEER: Unified Style Transfer with Expert Reinforcement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hallinan, Skyler and Brahman, Faeze and Lu, Ximing and Jung, Jaehun and Welleck, Sean and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="huang2023" class="col-sm-8"> <div class="title"><a href="" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;">Affective and Dynamic Beam Search for Story Generation </a></div> <div class="author"> Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Muhao Chen, and Snigdha Chaturved</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Affective and Dynamic Beam Search for Story Generation }</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Tenghao and Qasemi, Ehsan and Li, Bangzheng and Wang, He and Brahman, Faeze and Chen, Muhao and Chaturved, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NeurIPS</a></abbr></div> <div id="Lin2023SwiftSageAG" class="col-sm-8"> <div class="title"><a href="https://arxiv.org/abs/2305.17390" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</a></div> <div class="author"> Bill Yuchen Lin, Yicheng Fu, Karina Yang, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Shiyu Huang, Prithviraj Ammanabrolu, Chandra Bhagavatula, Yejin Choi, and Xiang Ren</div> <div class="periodical"> <em>NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/yuchenlin/SwiftSage" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent’s action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex real-world tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Lin2023SwiftSageAG</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Brahman, Faeze and Huang, Shiyu and Ammanabrolu, Prithviraj and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2305.17390}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2305.17390}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ACL</a></abbr></div> <div id="chen-etal-2023-rev" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2023.acl-long.112" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">REV: Information-Theoretic Evaluation of Free-Text Rationales</a></div> <div class="author"> Hanjie Chen, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Xiang Ren, Yangfeng Ji, Yejin Choi, and Swabha Swayamdipta</div> <div class="periodical"> <em>In Proceedings of ACL</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/HanjieChen/REV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen-etal-2023-rev</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{REV}: Information-Theoretic Evaluation of Free-Text Rationales}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Hanjie and Brahman, Faeze and Ren, Xiang and Ji, Yangfeng and Choi, Yejin and Swayamdipta, Swabha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of ACL}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.112}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.112}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2007--2030}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">ICLR</a></abbr></div> <div id="welleck2023generating" class="col-sm-8"> <div class="title"><a href="https://openreview.net/forum?id=hH36JeQZDaO" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Generating Sequences by Learning to Self-Correct</a></div> <div class="author"> Sean Welleck*, Ximing Lu*, Peter West+, Faeze Brahman +, Tianxiao Shen, Daniel Khashabi, and Yejin Choi</div> <div class="periodical"> <em>In The Eleventh International Conference on Learning Representations </em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">welleck2023generating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generating Sequences by Learning to Self-Correct}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Welleck*, Sean and Lu*, Ximing and West+, Peter and +, {Faeze Brahman} and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Eleventh International Conference on Learning Representations }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=hH36JeQZDaO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="jung-etal-2022-maieutic" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.emnlp-main.82" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</a></div> <div class="author"> Jaehun Jung, Lianhui Qin, Sean Welleck, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi</div> <div class="periodical"> <em>In Proceedings of EMNLP</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/jaehunjung1/Maieutic-Prompting" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jung-etal-2022-maieutic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Le Bras, Ronan and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.emnlp-main.82}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.emnlp-main.82}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1266--1279}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="vijjini-etal-2022-towards" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.emnlp-main.613" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Towards Inter-character Relationship-driven Story Generation</a></div> <div class="author"> Anvesh Rao Vijjini, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Proceedings of EMNLP</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In this paper, we introduce the task of modeling interpersonal relationships for story generation. For addressing this task, we propose Relationships as Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories sentence by sentence and has two major components - a relationship selector and a story continuer. The relationship selector specifies a latent variable to pick the relationship to exhibit in the next sentence and the story continuer generates the next sentence while expressing the selected relationship in a coherent way. Our automatic and human evaluations demonstrate that ReLiSt is able to generate stories with relationships that are more faithful to desired relationships while maintaining the content quality. The relationship assignments to sentences during inference brings interpretability to ReLiSt.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vijjini-etal-2022-towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Inter-character Relationship-driven Story Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vijjini, Anvesh Rao and Brahman, Faeze and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.emnlp-main.613}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.emnlp-main.613}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8970--8987}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NAACL</a></abbr></div> <div id="wnu-2022-narrative" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.wnu-1.0" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Proceedings of the 4th Workshop of Narrative Understanding (WNU2022)</a></div> <div class="author"> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@proceedings</span><span class="p">{</span><span class="nl">wnu-2022-narrative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Proceedings of the 4th Workshop of Narrative Understanding (WNU2022)}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Clark, Elizabeth and Brahman, Faeze and Iyyer, Mohit}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seattle, United States}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.wnu-1.0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NAACL</a></abbr></div> <div id="zhao-etal-2022-revisiting" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.findings-naacl.129" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach</a></div> <div class="author"> Chao Zhao, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Tenghao Huang, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Findings of NAACL</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Pre-trained models (PTMs) have lead to great improvements in natural language generation (NLG). However, it is still unclear how much commonsense knowledge they possess. With the goal of evaluating commonsense knowledge of NLG models, recent work has proposed the problem of generative commonsense reasoning, e.g., to compose a logical sentence given a set of unordered concepts. Existing approaches to this problem hypothesize that PTMs lack sufficient parametric knowledge for this task, which can be overcome by introducing external knowledge or task-specific pre-training objectives. Different from this trend, we argue that PTM’s inherent ability for generative commonsense reasoning is underestimated due to the order-agnostic property of its input. In particular, we hypothesize that the order of the input concepts can affect the PTM’s ability to utilize its commonsense knowledge. To this end, we propose a pre-ordering approach to elaborately manipulate the order of the given concepts before generation. Experiments show that our approach can outperform the more sophisticated models that have access to a lot of external data and resources.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhao-etal-2022-revisiting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Chao and Brahman, Faeze and Huang, Tenghao and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of NAACL}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seattle, United States}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-naacl.129}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.findings-naacl.129}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1709--1718}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="zhao-etal-2022-narrasum" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.findings-emnlp.14" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization</a></div> <div class="author"> Chao Zhao, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Kaiqiang Song, Wenlin Yao, Dian Yu, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/zhaochaocs/narrasum" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code/Data</a> </div> <div class="abstract hidden"> <p>Narrative summarization aims to produce a distilled version of a narrative to describe its most salient events and characters. Writing a summary for a narrative is challenging as it requires an understanding of event causality and character behaviors. To encourage research in this direction, we propose NarraSum, a large-scale narrative summarization dataset. It contains 122K narratives, which are collected from the synopses of movies and TV episodes with diverse genres, and their corresponding abstractive summaries. Experiments show that there is a large performance gap between humans and the state-of-the-art summarization models on NarraSum. We hope that this dataset will promote future research in summarization, as well as broader studies of natural language understanding and generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhao-etal-2022-narrasum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{N}arra{S}um: A Large-Scale Dataset for Abstractive Narrative Summarization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Chao and Brahman, Faeze and Song, Kaiqiang and Yao, Wenlin and Yu, Dian and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-emnlp.14}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.findings-emnlp.14}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{182--197}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/zhaochaocs/narrasum}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="brahman-etal-2022-grounded" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2022.findings-emnlp.547" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Baolin Peng, Michel Galley, Sudha Rao, Bill Dolan, Snigdha Chaturvedi, and Jianfeng Gao</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/fabrahman/Grounded_Keys2Text" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code/Data</a> </div> <div class="abstract hidden"> <p>Large pre-trained language models have recently enabled open-ended generation frameworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond the traditional data-to-text generation. While this framework is more general, it is under-specified and often leads to a lack of controllability restricting their real-world usage. We propose a new grounded keys-to-text generation task: the task is to generate a factual description about an entity given a set of guiding keys, and grounding passages. To address this task, we introduce a new dataset, called EntDeGen. Inspired by recent QA-based evaluation measures, we propose an automatic metric, MAFE, for factual correctness of generated descriptions. Our EntDescriptor model is equipped with strong rankers to fetch helpful passages and generate entity descriptions. Experimental result shows a good correlation (60.14) between our proposed metric and human judgments of factuality. Our rankers significantly improved the factual correctness of generated descriptions (15.95% and 34.51% relative gains in recall and precision). Finally, our ablation study highlights the benefit of combining keys and groundings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">brahman-etal-2022-grounded</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brahman, Faeze and Peng, Baolin and Galley, Michel and Rao, Sudha and Dolan, Bill and Chaturvedi, Snigdha and Gao, Jianfeng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-emnlp.547}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.findings-emnlp.547}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7397--7413}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/fabrahman/Grounded_Keys2Text}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">NAACL</a></abbr></div> <div id="nuse-2021-narrative" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2021.nuse-1.0" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Proceedings of the Third Workshop on Narrative Understanding</a></div> <div class="author"> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@proceedings</span><span class="p">{</span><span class="nl">nuse-2021-narrative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Proceedings of the Third Workshop on Narrative Understanding}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Akoury, Nader and Brahman, Faeze and Chaturvedi, Snigdha and Clark, Elizabeth and Iyyer, Mohit and Martin, Lara J.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Virtual}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.nuse-1.0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">TACL</a></abbr></div> <div id="khashabi-etal-2021-parsinlu" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2021.tacl-1.68" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">ParsiNLU: A Suite of Language Understanding Challenges for Persian</a></div> <div class="author"> Daniel Khashabi, Arman Cohan, Siamak Shakeri, Pedram Hosseini, Pouya Pezeshkpour, Malihe Alikhani, Moin Aminnaseri, Marzieh Bitaab, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Sarik Ghazarian, Mozhdeh Gheini, Arman Kabiri, Rabeeh Karimi Mahabagdi, Omid Memarrast, Ahmadreza Mosallanezhad, Erfan Noury, Shahab Raji, Mohammad Sadegh Rasooli, Sepideh Sadeghi, Erfan Sadeqi Azer, Niloofar Safi Samghabadi, Mahsa Shafaei, Saber Sheybani, Ali Tazarv, and Yadollah Yaghoobzadeh</div> <div class="periodical"> <em>Transactions of the Association for Computational Linguistics</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Despite the progress made in recent years in addressing natural language understanding (NLU) challenges, the majority of this progress remains to be concentrated on resource-rich languages like English. This work focuses on Persian language, one of the widely spoken languages in the world, and yet there are few NLU datasets available for this language. The availability of high-quality evaluation datasets is a necessity for reliable assessment of the progress on different NLU tasks and domains. We introduce ParsiNLU, the first benchmark in Persian language that includes a range of language understanding tasks—reading comprehension, textual entailment, and so on. These datasets are collected in a multitude of ways, often involving manual annotations by native speakers. This results in over 14.5k new instances across 6 distinct NLU tasks. Additionally, we present the first results on state-of-the-art monolingual and multilingual pre-trained language models on this benchmark and compare them with human performance, which provides valuable insights into our ability to tackle natural language understanding challenges in Persian. We hope ParsiNLU fosters further research and advances in Persian language understanding.1</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">khashabi-etal-2021-parsinlu</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{P}arsi{NLU}: A Suite of Language Understanding Challenges for {P}ersian}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khashabi, Daniel and Cohan, Arman and Shakeri, Siamak and Hosseini, Pedram and Pezeshkpour, Pouya and Alikhani, Malihe and Aminnaseri, Moin and Bitaab, Marzieh and Brahman, Faeze and Ghazarian, Sarik and Gheini, Mozhdeh and Kabiri, Arman and Mahabagdi, Rabeeh Karimi and Memarrast, Omid and Mosallanezhad, Ahmadreza and Noury, Erfan and Raji, Shahab and Rasooli, Mohammad Sadegh and Sadeghi, Sepideh and Azer, Erfan Sadeqi and Samghabadi, Niloofar Safi and Shafaei, Mahsa and Sheybani, Saber and Tazarv, Ali and Yaghoobzadeh, Yadollah}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cambridge, MA}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MIT Press}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.tacl-1.68}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/tacl_a_00419}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1147--1162}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="brahman-etal-2021-characters-tell" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2021.findings-emnlp.150" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">“Let Your Characters Tell Their Story”: A Dataset for Character-Centric Narrative Understanding</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Meng Huang, Oyvind Tafjord, Chao Zhao, Mrinmaya Sachan, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://users.soe.ucsc.edu/%C2%A0hannahbrahman/dataset/liscu.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code/Data</a> </div> <div class="abstract hidden"> <p>When reading a literary piece, readers often make inferences about various characters’ roles, personalities, relationships, intents, actions, etc. While humans can readily draw upon their past experiences to build such a character-centric view of the narrative, understanding characters in narratives can be a challenging task for machines. To encourage research in this field of character-centric narrative understanding, we present LiSCU – a new dataset of literary pieces and their summaries paired with descriptions of characters that appear in them. We also introduce two new tasks on LiSCU: Character Identification and Character Description Generation. Our experiments with several pre-trained language models adapted for these tasks demonstrate that there is a need for better models of narrative comprehension.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">brahman-etal-2021-characters-tell</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{``}Let Your Characters Tell Their Story{''}: A Dataset for Character-Centric Narrative Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brahman, Faeze and Huang, Meng and Tafjord, Oyvind and Zhao, Chao and Sachan, Mrinmaya and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Punta Cana, Dominican Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.findings-emnlp.150}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.findings-emnlp.150}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1734--1752}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://users.soe.ucsc.edu/~hannahbrahman/dataset/liscu.html}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="huang-etal-2021-uncovering-implicit" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2021.findings-emnlp.326" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Uncovering Implicit Gender Bias in Narratives through Commonsense Inference</a></div> <div class="author"> Tenghao Huang, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Vered Shwartz, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Findings of EMNLP</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly (“women can’t park”) or implicitly (e.g. an unsolicited male character guides her into a parking space). We focus on implicit biases, and use a commonsense reasoning engine to uncover them. Specifically, we infer and analyze the protagonist’s motivations, attributes, mental states, and implications on others. Our findings regarding implicit biases are in line with prior work that studied explicit biases, for example showing that female characters’ portrayal is centered around appearance, while male figures’ focus on intellect.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang-etal-2021-uncovering-implicit</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Uncovering Implicit Gender Bias in Narratives through Commonsense Inference}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Tenghao and Brahman, Faeze and Shwartz, Vered and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Punta Cana, Dominican Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.findings-emnlp.326}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.findings-emnlp.326}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3866--3873}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="basu-roy-chowdhury-etal-2021-everything" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2021.emnlp-main.841" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Is Everything in Order? A Simple Way to Order Sentences</a></div> <div class="author"> Somnath Basu Roy Chowdhury, <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Proceedings of EMNLP</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/fabrahman/ReBART" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.cs.unc.edu/%C2%A0somnath/files/ReBART-Poster.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>The task of organizing a shuffled set of sentences into a coherent text has been used to evaluate a machine’s understanding of causal and temporal relations. We formulate the sentence ordering task as a conditional text-to-marker generation problem. We present Reorder-BART (Re-BART) that leverages a pre-trained Transformer-based model to identify a coherent order for a given set of shuffled sentences. The model takes a set of shuffled sentences with sentence-specific markers as input and generates a sequence of position markers of the sentences in the ordered text. Re-BART achieves the state-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and Kendall’s tau. We perform evaluations in a zero-shot setting, showcasing that our model is able to generalize well across other datasets. We additionally perform several experiments to understand the functioning and limitations of our framework.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">basu-roy-chowdhury-etal-2021-everything</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Is Everything in Order? A Simple Way to Order Sentences}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Basu Roy Chowdhury, Somnath and Brahman, Faeze and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online and Punta Cana, Dominican Republic}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.emnlp-main.841}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.emnlp-main.841}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10769--10779}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">AAAI</a></abbr></div> <div id="brahman2021learning" class="col-sm-8"> <div class="title"><a href="" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;">Learning to rationalize for nonmonotonic reasoning with distant supervision</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Vered Shwartz, Rachel Rudinger, and Yejin Choi</div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/fabrahman/RationaleGen" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="http://alumni.soe.ucsc.edu/%C2%A0hannahbrahman/AAAI2021_slides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>The black-box nature of neural models has motivated a line of research that aims to generate natural language rationales to explain why a model made certain predictions. Such rationale generation models, to date, have been trained on dataset-specific crowdsourced rationales, but this approach is costly and is not generalizable to new tasks and domains. In this paper, we investigate the extent to which neural models can reason about natural language rationales that explain model predictions, relying only on distant supervision with no additional annotation cost for human-written rationales. We investigate multiple ways to automatically generate rationales using pre-trained language models, neural knowledge models, and distant supervision from related tasks, and train generative models capable of composing explanatory rationales for unseen instances. We demonstrate our approach on the defeasible inference task, a nonmonotonic reasoning task in which an inference may be strengthened or weakened when new information (an update) is introduced. Our model shows promises at generating post-hoc rationales explaining why an inference is more or less likely given the additional information, however, it mostly generates trivial rationales reflecting the fundamental limitations of neural language models. Conversely, the more realistic setup of jointly predicting the update or its type and generating rationale is more challenging, suggesting an important future direction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">brahman2021learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to rationalize for nonmonotonic reasoning with distant supervision}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brahman, Faeze and Shwartz, Vered and Rudinger, Rachel and Choi, Yejin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12592--12601}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="brahman-etal-2020-cue" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2020.aacl-main.59" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Cue Me In: Content-Inducing Approaches to Interactive Story Generation</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, Alexandru Petrusca, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Automatically generating stories is a challenging problem that requires producing causally related and logical sequences of events about a topic. Previous approaches in this domain have focused largely on one-shot generation, where a language model outputs a complete story based on limited initial input from a user. Here, we instead focus on the task of interactive story generation, where the user provides the model mid-level sentence abstractions in the form of cue phrases during the generation process. This provides an interface for human users to guide the story generation. We present two content-inducing approaches to effectively incorporate this additional information. Experimental results from both automatic and human evaluations show that these methods produce more topically coherent and personalized stories compared to baseline methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#c869bf"><a href="">EMNLP</a></abbr></div> <div id="brahman-chaturvedi-2020-modeling" class="col-sm-8"> <div class="title"><a href="https://aclanthology.org/2020.emnlp-main.426" target="_blank" style="color: rgb(0, 0, 0);font: size 14px; font-weight: 400;" rel="external nofollow noopener">Modeling Protagonist Emotions for Emotion-Aware Storytelling</a></div> <div class="author"> <span style="color: rgb(150, 87, 182);font-weight:400">Faeze Brahman</span>, and Snigdha Chaturvedi</div> <div class="periodical"> <em>In Proceedings of EMNLP</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/fabrahman/Emo-Aware-Storytelling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="http://alumni.soe.ucsc.edu/%C2%A0hannahbrahman/EMNLP_slides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="abstract hidden"> <p>Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">brahman-chaturvedi-2020-modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling Protagonist Emotions for Emotion-Aware Storytelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Brahman, Faeze and Chaturvedi, Snigdha}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EMNLP}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.emnlp-main.426}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2020.emnlp-main.426}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5277--5294}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Faeze Brahman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: May 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>